# .env

# --- Data Paths ---
MIDI_FILES_DIR=../.data/midis
PREPROCESSED_DATA_DIR=../.data/preprocessed
MODELS_OUTPUT_DIR=../.data/models/music2midi

# --- Data Preprocessing ---
# Duration in seconds to chop MIDI files. Whisper's context is 30s.
CHUNK_DURATION_SECONDS=10
# The sample rate Whisper expects. Do not change.
SAMPLE_RATE=16000
# Path to a high-quality SoundFont file for MIDI rendering.
# Download one if you don't have it (e.g., from https://sites.google.com/site/soundfonts4u/)
SOUNDFONT_PATH=/usr/share/sounds/sf2/FluidR3_GM.sf2
# Target vocabulary size for BPE tokenizer training
BPE_VOCAB_SIZE=2000

# --- Model Configuration ---
WHISPER_MODEL="openai/whisper-base"
QWEN_MODEL="Qwen/Qwen3-0.6B-Base"
# Number of top decoder layers in Qwen to unfreeze for fine-tuning.
TOP_K_QWEN_LAYERS=4

# --- Training Hyperparameters ---
BATCH_SIZE=4
EPOCHS=10
# Learning rate for the newly added parts (e.g., projection layer if we add one)
LR_ADAPTER=1e-4
# Learning rate for the unfrozen Qwen layers. Should be smaller.
LR_QWEN=2e-5
WEIGHT_DECAY=1e-4
# Seed for reproducibility
SEED=42

# --- W&B Logging ---
WANDB_API_KEY=...
WANDB_ENTITY=charles-cai
# A new project name for this experiment
WANDB_PROJECT=mlx8-w5-music-transcription

# --- Inference ---
# Path to a specific model checkpoint to load for inference.
# Example: ../.data/models/music2midi/checkpoint_epoch_10.pt
INFERENCE_CHECKPOINT_PATH=
# Path to a .wav or .mp3 file to transcribe.
INFERENCE_AUDIO_FILE=

BATCH_SIZE_MUSIC2MIDI=8
EPOCHS_MUSIC2MIDI=3

